#!/usr/bin/env python3
#
# Copyright (c) 2018 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Author: William Henning <whenning@google.com>
#
# This script parses the validation layers test continuous integration ouput
# and reports the number of tests that passed, failured, ouput unexpected errors,
# or were skipped. As such, the script is only designed to parse the ouput
# generated by the existing CI implementation.
#
# usage:
#       for profile in tests/device_profiles/*.json; do echo Testing with
#       profile $profile; VK_LAYER_PATH=DEVSIM_AND_VALIDATION_PATHS
#       VK_DEVSIM_FILE=$profile VK_ICD_FILENAMES=MOCK_ICD_PATH
#       ./build/tests/vk_layer_validation_tests --devsim; done
#       | python3 parse_test_results.py [--fail_on_skip] [--fail_on_unexpected]
#
#       --fail_on_skip causes the script to exit with a non-zero exit code if a test
#       didn't run on any device profile
#
#       --fail_on_unexpected causes the script to exit with a non-zero exit code if
#       a test printed unexpected errors
#

from enum import Enum
import argparse
import re
import sys
from collections import defaultdict

class Result(Enum):
  UNKNOWN = -1
  PASSED = 0
  SKIPPED = 1
  UNEXPECTED = 2
  FAILED = 3

class OutputStats(object):
    def __init__(self):
        self.current_profile = ""
        self.current_test = ""
        self.current_test_output = ""
        self.test_results = defaultdict(defaultdict)

    def match(self, line):
        self.new_profile_match(line)
        self.test_suite_end_match(line)
        self.start_test_match(line)
        if self.current_test != "":
            self.current_test_output += line
        self.skip_test_match(line)
        self.pass_test_match(line)
        self.fail_test_match(line)
        self.unexpected_error_match(line)

    def print_summary(self, skip_is_failure, unexpected_is_failure):
        if self.current_test != "":
            self.test_died()

        test_count = defaultdict(int)
        outcome = Result.PASSED

        for test_name, results in self.test_results.items():
            profile_count = defaultdict(int)
            for profile, result in results.items():
                profile_count[result] += 1

            if profile_count[Result.FAILED] != 0:
                print("TEST FAILED:", test_name)
                test_count[Result.FAILED] += 1
            elif profile_count[Result.SKIPPED] == len(results):
                print("TEST SKIPPED ALL DEVICES:", test_name)
                test_count[Result.SKIPPED] += 1
            elif profile_count[Result.UNEXPECTED] != 0:
                print("UNEXPECTED ERRORS:", test_name)
                test_count[Result.UNEXPECTED] += 1
            else:
                test_count[Result.PASSED] += 1

        num_tests = len(self.test_results)
        print("PASSED: ", test_count[Result.PASSED], "/", num_tests, " tests")
        if test_count[Result.UNEXPECTED] != 0:
            if unexpected_is_failure:
                outcome = Result.UNEXPECTED
            print("UNEXPECTED OUPUT: ", test_count[Result.UNEXPECTED], "/", num_tests, "tests")

        if test_count[Result.SKIPPED] != 0:
            if skip_is_failure:
                outcome = Result.SKIPPED
            print("NEVER RAN: ", test_count[Result.SKIPPED], "/", num_tests, " tests")

        if test_count[Result.FAILED] != 0:
            outcome = Result.FAILED
            print("FAILED: ", test_count[Result.FAILED], "/", num_tests, "tests")
        return outcome

    def new_profile_match(self, line):
        new_profile_match = re.search(r'Testing with profile .*/(.*)', line)
        if new_profile_match != None:
            self.current_profile = new_profile_match.group(1)

    def test_suite_end_match(self, line):
        if re.search(r'\[-*\]', line) != None:
            if self.current_test != "":
                # Here we see a message that starts [----------] before another test
                # finished running. This should mean that that other test died.
                self.test_died()

    def start_test_match(self, line):
        if re.search(r'\[ RUN\s*\]', line) != None:
            # This parser doesn't handle the case where one test's start comes between another
            # test's start and result.
            assert self.current_test == ""
            self.current_test = re.search(r'] (.*)', line).group(1)
            self.current_test_output = ""

    def current_result(self):
        return self.test_results.get(self.current_test, {}).get(self.current_profile, Result.UNKNOWN)

    def set_current_result(self, result):
        self.test_results[self.current_test][self.current_profile] = result

    def skip_test_match(self, line):
        if re.search(r'TEST SKIPPED', line) != None:
            self.set_current_result(Result.SKIPPED);

    def pass_test_match(self, line):
        if re.search(r'\[\s*OK \]', line) != None:
            # If gtest says the test passed, check if it was skipped or printed
            # unexpected output before marking it as passed.
            if self.current_result() != Result.SKIPPED and self.current_result() != Result.UNEXPECTED:
                self.set_current_result(Result.PASSED)
            self.current_test = ""

    def fail_test_match(self, line):
        if re.search(r'\[\s*FAILED\s*\]', line) != None and self.current_test != "":
            self.set_current_result(Result.FAILED)
            self.current_test = ""

    def unexpected_error_match(self, line):
        if re.search(r'^Unexpected: ', line) != None:
            self.set_current_result(Result.UNEXPECTED)

    def test_died(self):
        print("A test likely crashed. Testing is being aborted.")
        sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description='Parse the output from validation layer tests.')
    parser.add_argument('--fail_on_skip', action='store_true', help="Makes the script exit with a "
                        "non-zero exit code if a test didn't run on any device profile.")
    parser.add_argument('--fail_on_unexpected', action='store_true', help="Makes the script exit "
                        "with a non-zero exit code if a test causes unexpected errors.")
    args = parser.parse_args()

    stats = OutputStats()
    for line in sys.stdin:
        stats.match(line)
    outcome = stats.print_summary(args.fail_on_skip, args.fail_on_unexpected)
    if outcome != Result.PASSED:
        print("")
        if outcome == Result.SKIPPED:
            print("Failed CI because one or more tests skipped on every device profile.\n")
        elif outcome == Result.UNEXPECTED:
            print("Failed CI because one or more tests caused unexpected errors in validation.\n")
        elif outcome == Result.FAILED:
            print("Failed CI because one or more tests failed.\n")
        sys.exit(1)

if __name__ == '__main__':
    main()
